{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25873657-d4fe-4ff4-b3c0-a30c245ec12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from common import *\n",
    "from mcs_shared import (\n",
    "    ACCUMULATION_FLIGHTS, SnotelPointData,\n",
    "    load_als_depth, load_factors_tif, load_isnobal_depth,\n",
    "    get_station_pixel_factors, get_station_pixel_depths\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "use_hvplot()\n",
    "\n",
    "RESOLUTION = 100 # meters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607a391-8712-4c91-b5dc-557ef4aa9f9b",
   "metadata": {},
   "source": [
    "## ALS depth\n",
    "### Normalize by median depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762c639-455f-479e-ba09-7543ec7d155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "als_patterns = {\n",
    "    flight: load_als_depth(flight, RESOLUTION, base_run=True) for flight in ACCUMULATION_FLIGHTS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b87dff-ee9a-4dc2-8b91-6c8060096876",
   "metadata": {},
   "source": [
    "#### Excude outliers outside 1st, 99th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9f338-c6d9-4601-889f-fe3243a495f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date, depth in als_patterns.items():\n",
    "    als_patterns[date] = np.clip(\n",
    "        depth, \n",
    "        a_min=np.nanpercentile(depth, 1),\n",
    "        a_max=np.nanpercentile(depth, 99)\n",
    "    )\n",
    "    # als_patterns[date] = np.where((depth < np.nanpercentile(depth, 1)) | (depth > np.nanpercentile(depth, 99)), np.nan, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951f02c-a0d7-4d20-8c11-e425b09d797b",
   "metadata": {},
   "source": [
    "#### Normalize by median\n",
    "\n",
    "Robust scaling with median and inter-quartile range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d3d64-084e-49de-b83c-00a05265023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date, depth in als_patterns.items():\n",
    "    median = np.nanmedian(depth)\n",
    "    q1 = np.nanpercentile(depth, 1)\n",
    "    q3 = np.nanpercentile(depth, 99)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # als_patterns[date] = (\n",
    "    #     (depth - median) / iqr\n",
    "    # ) + 1\n",
    "    als_patterns[date] = depth/median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24bf70-132f-476f-9af1-194eab9cfcf9",
   "metadata": {},
   "source": [
    "#### Test for skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc89b7e-167b-46a9-87d2-3d85062bbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min = 1\n",
    "\n",
    "for date, factor in als_patterns.items():\n",
    "    print(date)\n",
    "    data = factor[~np.isnan(factor)]\n",
    "    skew = stats.skew(data)\n",
    "    global_min = np.minimum(global_min, np.min(data))\n",
    "\n",
    "    print(f\"Skew: {skew:.4f}\")\n",
    "    print(f\"Max: {np.nanmax(data):.4f}\")\n",
    "    print(f\"Min: {np.nanmin(data):.4f}\")\n",
    "    print(\"==\")\n",
    "\n",
    "print(f\"\\nGlobal min: {global_min:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1533e-5a88-4c0f-ae10-29cedfebcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(data, label, color=\"black\"):\n",
    "    hist_data = data.flatten()\n",
    "    hist_data = hist_data[~np.isnan(hist_data)]\n",
    "    line_width = 1\n",
    "    if color == \"green\":\n",
    "        line_width = 2\n",
    "    \n",
    "    return hv.Distribution(hist_data, label=label).opts(filled=False, width=800, height=600, tools=['hover'], line_color=color, line_width=line_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a16e8-1a6e-4a92-8c7b-7bccedaa1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Overlay(\n",
    "    [plot_dist(pattern, date) for date, pattern in als_patterns.items()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38b9a7-3e08-4cc8-a1e7-1ab7846fc521",
   "metadata": {},
   "source": [
    "### Extract one pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1c6aa-cb4f-4cc8-b9fa-6df162bb2c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_norm_pattern(arrays):\n",
    "    \"\"\"\n",
    "    Extracts a single summary array using\n",
    "    Median-Based Quantile Normalization\n",
    "    \"\"\"\n",
    "    # Flatten each array and remove any NaN values\n",
    "    flat_data = [arr[~np.isnan(arr)] for arr in arrays]\n",
    "    \n",
    "    # Build quantiles for interpolation\n",
    "    quantiles = np.linspace(0, 1, 10_000)\n",
    "\n",
    "    # Build reference distribution\n",
    "    ref_dist = np.median(\n",
    "        [\n",
    "            np.quantile(d, quantiles) for d in flat_data\n",
    "        ], \n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    # Map each input array to reference\n",
    "    norm_arrays = []\n",
    "    for a in arrays:\n",
    "        norm_a = a.copy().astype(float)\n",
    "        mask = ~np.isnan(a)\n",
    "        values = a[mask]\n",
    "        \n",
    "        # Calculate Ranks \n",
    "        ranks = np.argsort(np.argsort(values))\n",
    "        \n",
    "        # Convert ranks to percentiles\n",
    "        rel_ranks = ranks / (len(values) - 1)\n",
    "        \n",
    "        # Map to the Median Reference Distribution via linear interpolation\n",
    "        norm_a[mask] = np.interp(rel_ranks, quantiles, ref_dist)\n",
    "        norm_arrays.append(norm_a) \n",
    "        \n",
    "        \n",
    "        # mask = ~np.isnan(a)\n",
    "        \n",
    "        # ranks = np.argsort(np.argsort(a[mask]))\n",
    "        \n",
    "        # # Scale to reference\n",
    "        # indices = (ranks * (len(ref_dist)-1) / (len(ranks)-1)).astype(int)\n",
    "        \n",
    "        # # Create pattern\n",
    "        # new_a = np.full(a.shape, np.nan)\n",
    "        # new_a[mask] = ref_dist[indices]\n",
    "        # norm_arrays.append(new_a)\n",
    "\n",
    "    # Final pixel-wise median\n",
    "    return np.nanmedian(np.stack(norm_arrays), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeedc8e2-9c07-4a66-9b5f-fc06c5adcfaf",
   "metadata": {},
   "source": [
    "## QQ norm pattern across seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d3b1a-7422-49e6-8df4-50c0a47d8eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "als_factor_norm = quantile_norm_pattern(als_patterns.values())\n",
    "als_factor_norm[als_factor_norm <= 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2b546-51b7-4bae-8944-e960e413b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(als_factor_norm == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110df437-e43c-4cba-bf64-6d24e51f8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Image(als_factor_norm).opts(\n",
    "    width=800, height=800, aspect='equal', \n",
    "    cmap='RdBu', clim=(0, 2), \n",
    "    colorbar=True, tools=['hover'], hover_tooltips=[ (\"Factor\", \"@image{0.2f}\") ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1529f-30c6-4f9b-8eaa-b7e17736db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Overlay(\n",
    "    [\n",
    "        plot_dist(pattern, date) \n",
    "        for date, pattern in als_patterns.items()\n",
    "    ] + \n",
    "    [\n",
    "        plot_dist(als_factor_norm, \"qq\", \"green\")\n",
    "    ] +\n",
    "    [\n",
    "        plot_dist(np.nanmedian(np.stack(list(als_patterns.values())), axis=0), \"median\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa03af-8e04-4919-b44f-ba27a8af6987",
   "metadata": {},
   "source": [
    "## Smooth and save via GDAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e038a6-4064-4c16-b726-90eba045b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from osgeo import gdal\n",
    "\n",
    "gdal.UseExceptions()\n",
    "driver = gdal.GetDriverByName('GTiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d2571-0564-435c-b1fd-d6653618c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gdal.Open(\n",
    "    f\"/bsushare/hpmarshall-shared/jmeyer/MCS-ALS-snowdepth/{RESOLUTION}m_base/MCS_REFDEM_32611_{RESOLUTION}m.tif\", \n",
    "    gdal.GA_ReadOnly\n",
    ") as src_ds:\n",
    "    orig_file = driver.CreateCopy('/vsimem/orig_%i.tif' % random.getrandbits(32), src_ds)\n",
    "\n",
    "out_band = orig_file.GetRasterBand(1)\n",
    "out_band.WriteArray(als_factor_norm)\n",
    "out_band.SetNoDataValue(float(np.nan))\n",
    "out_band.FlushCache()\n",
    "\n",
    "# QA output\n",
    "qq_file = gdal.Translate(\n",
    "    f\"/bsushare/hpmarshall-shared/jmeyer/MCS-ALS-snowdepth/precip_factors/MCS_pattern_{RESOLUTION}m_qq_norm.tif\",\n",
    "    orig_file\n",
    ")\n",
    "qq_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac9fd3-24eb-4352-abc5-9776851a41d1",
   "metadata": {},
   "source": [
    "Ensure \"unknown\" precip factors don't erase values in the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3814f10c-0074-46ba-af92-085aa9c20372",
   "metadata": {},
   "outputs": [],
   "source": [
    "als_factor_norm[np.isnan(als_factor_norm)] = 1\n",
    "\n",
    "out_band = orig_file.GetRasterBand(1)\n",
    "out_band.WriteArray(als_factor_norm)\n",
    "out_band.SetNoDataValue(float(np.nan))\n",
    "out_band.FlushCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d840cb-d157-40ed-9d75-85e6cc77a204",
   "metadata": {},
   "source": [
    "#### Below is only for 10m resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e5032-a508-4fbd-9e5e-195d0432a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESOLUTION == 10:\n",
    "    # Filled gaps in 10m source using the average algorithm\n",
    "    filled_file = '/vsimem/filled_%i.tif' % random.getrandbits(32)\n",
    "    filled_options = gdal.TranslateOptions(\n",
    "        xRes=1, yRes=1,\n",
    "        resampleAlg=gdal.GRA_Average,\n",
    "    )\n",
    "    filled_ds = gdal.Translate(filled_file, orig_file, options=filled_options)\n",
    "    \n",
    "    # Smooth to 100m for length scale\n",
    "    smooth_file = '/vsimem/smooth_%i.tif' % random.getrandbits(32)\n",
    "    smooth_options = gdal.WarpOptions(\n",
    "        xRes=100, yRes=100,\n",
    "        resampleAlg=gdal.GRA_Med,\n",
    "    )\n",
    "    smooth_ds = gdal.Warp(smooth_file, filled_ds , options=smooth_options)\n",
    "else:\n",
    "    smooth_ds = orig_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371446e-ae60-46dc-9603-935b04869959",
   "metadata": {},
   "source": [
    "### For both resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6dbb19-92d8-4f29-8a5c-d0fb7811a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final map at model native resolution\n",
    "output_file = f\"/bsushare/hpmarshall-shared/jmeyer/MCS-ALS-snowdepth/precip_factors/MCS_pattern_{RESOLUTION}m.tif\"\n",
    "pattern_options = gdal.WarpOptions(\n",
    "    # Target extent: [minX, maxY, maxX, minY]\n",
    "    outputBounds=[594356.438, 4855619.000, 616456.438, 4877419.000],\n",
    "    xRes=10, yRes=10,\n",
    "    resampleAlg=gdal.GRA_Cubic,\n",
    "    errorThreshold=0,\n",
    ")\n",
    "# Save final map to disk\n",
    "saved_file = gdal.Warp(output_file, smooth_ds, options=pattern_options)\n",
    "\n",
    "filled_ds = None\n",
    "smooth_ds = None\n",
    "saved_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16147aa-08b2-4989-828d-de2e6ea57274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pattern(file):\n",
    "    with gdal.Open(file, gdal.GA_ReadOnly) as file:\n",
    "        pattern_band = file.GetRasterBand(1)\n",
    "        pattern_data = pattern_band.ReadAsArray()\n",
    "\n",
    "    return pattern_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f67065-933c-4c4c-8571-97304debc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Image(read_pattern(output_file)).opts(\n",
    "    height=800, width=800, aspect='equal', \n",
    "    cmap='PuOr', clim=(0, 2), \n",
    "    tools=[\"hover\"], colorbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb376039-5be6-4649-89a6-8e80e69fb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist(read_pattern(output_file), \"100m\") * plot_dist(als_factor_norm, \"qq\", \"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdff91-35da-45b9-ac6c-00408b1c47da",
   "metadata": {},
   "source": [
    "## Apply factors to the NetCDF files\n",
    "\n",
    "Ensure that we are using the \"base\" run precip files\n",
    "```\n",
    "rsync -av --include=\"*/\" --include=\"precip.nc\" --exclude=\"*\" mcs_base/ mcs/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13910f88-e5f9-4d9b-ad2f-8a0db27fa0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import dask\n",
    "import glob\n",
    "\n",
    "from dask_utils import run_with_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a90bc-b68b-4f6a-bd7a-51da9eabb8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def scale_precip(file_path, factor_file):\n",
    "    # Same as above method pasted for each worker\n",
    "    with gdal.Open(factor_file, gdal.GA_ReadOnly) as file:\n",
    "        band = file.GetRasterBand(1)\n",
    "        factors = band.ReadAsArray()\n",
    "    \n",
    "        band = None\n",
    "\n",
    "    with nc.Dataset(file_path, 'r+') as ds:\n",
    "        ds.set_auto_mask(False)\n",
    "        precip = ds.variables[\"precip\"]\n",
    "        \n",
    "        data = precip[:]\n",
    "        data = data * factors\n",
    "        data[np.isnan(data)] = 0.\n",
    "\n",
    "        precip[:] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed0ef4-a114-4925-8c4c-c04f1196be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with run_with_client(10, 60) as client:\n",
    "    files = glob.glob(\"/bsushare/hpmarshall-shared/jmeyer/iSnobal/MCS/isnobal/wy2025/mcs/*/precip.nc\")\n",
    "    jobs = [scale_precip(f, output_file) for f in files]\n",
    "\n",
    "    dask.compute(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827a60b-1f76-4ece-b65e-8e936c000507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Snow Viz",
   "language": "python",
   "name": "snow_viz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
